                                                             February 26th, 2024

It's been a while since I wrote a diary, but I am also not a complete stranger
to it.  Like, whenever I was learning something new, I would lead a diary to
see which obstacles I face and how I, hopefully, overcome them.

At this moment, the latest check-in is 10273831c78b1d18a1b87dc7f56b9c6d0d8b7787
it is a couple of check-ins after the first time I got credible results for
Navier-Stokes equatins, which was: bf7b654e0f58f9dc99faa99219be2da9a5622879

The current situation is that the tests from 1 to 5 work well on GPUs, show a
good speed-up, but test 6 is without any GPU functionality.  The reason is not
that wasn't done before, it was, but the implementation was a bit of a mess, as
if I was frentically inserting GPU (OpenACC) commands just to get any speed-up.
There was no real substance, the analithical thinking was absent.

Hence, the goal today, maybe the goal from this point on, would be to go slowly,
one step at a time, browsing profiling data and searching for opportunitues to
speed the code up.

Task 1: Find a case (which in current state of the code development means:
modify the sources in such a way to make the pressure solution the most CPU-
intensive part of the code, because that's how things normally are in CFD.
So not some case which converges after a couple of dozens of time steps and
then uses CPU time to save results or do something silly, something more sub-
stantial, more relevant to a typical CFD simulation.

In order to achieve that, I did the following changes to Test_006.f90:

-  integer, parameter       :: N_STEPS = 600 ! spend enough time on device
-  integer, parameter       :: N_ITERS =   6 ! spend enough time on device
+  integer, parameter       :: N_STEPS =   3 ! spend enough time on device
+  integer, parameter       :: N_ITERS =   3 ! spend enough time on device

Changed the number of time steps to 3 only, with 3 iterations each, but with
the grid size of 200^3.  In other words, changes in test_006_cube.ini read:

-grid_nx   40
-grid_ny   40
-grid_nz   40
+grid_nx  200
+grid_ny  200
+grid_nz  200

Since the program does not have any under-relaxation, time step is limited to
roughly 1 / nx, so in this case, another modification in Test_006.f90 is:

-  dt = 0.025
+  dt = 1.0 / 200.0

I compile the program with:

make GPU=no

This will invoke Nvidia Fortran compiler, and the profiling results read:

 #=================================================================================#
 #                              CPU usage statistics                               #
 #---------------------------------------------------------------------------------#
 #                      Total CPU time: 000:05:59 [hhh:mm:ss]                      #
 #---------------------------------------------+-----------------------------------#
 #        Description of the activity:         |            Spent time:            #
 #---------------------------------------------+-----------------+-----------------#
 # - CG_for_Pressure                           |   236.19 [s]    |     65.64 %     #
 # - Grad_Pressure                             |    78.26 [s]    |     21.75 %     #
 # - Save_Vtk_Vector                           |    13.20 [s]    |      3.67 %     #
 # - CG_for_Momentum                           |     9.34 [s]    |      2.59 %     #
 # - Save_Vtk_Scalar                           |     4.65 [s]    |      1.29 %     #
 # - Test_006                                  |     4.41 [s]    |      1.23 %     #
 # - Insert_Diffusion_Bc                       |     3.50 [s]    |      0.97 %     #
 # - Insert_Volume_Source_For_Pressure         |     3.28 [s]    |      0.91 %     #
 # - Correct_Velocity                          |     3.14 [s]    |      0.87 %     #
 # - Add_Advection_Term                        |     2.42 [s]    |      0.67 %     #
 # - Form_Diffusion_Matrix                     |     0.42 [s]    |      0.12 %     #
 # - Add_Pressure_Term                         |     0.32 [s]    |      0.09 %     #
 # - Add_Inertial_Term                         |     0.32 [s]    |      0.09 %     #
 # - Calculate_Grad_Matrix                     |     0.24 [s]    |      0.07 %     #
 # - Form_Pressure_Matrix                      |     0.15 [s]    |      0.04 %     #
 #---------------------------------------------+-----------------+-----------------#

This looks good, because CG for pressure takes 66% of the time.  That also
completes the Task 1.

Task 2: Decide what you want to port to GPU's (device).  At this point, it is a
no brainer.  One should port the solution of the CG for pressure to device.
In order to achieve that, one should transfer the pressure matrix, pressure
correction variable and also the source to "device" and see how the solution
of pressure correction alone speeds the overal computation up.

But, there is a catch with that.  In the code, OpenACC directives are inserted
in more places than just CG solver for pressure.  Solver for momentum equations
is the same as the one for pressure, so as soon as the program is compiled with
directives for OpenACC, necessary data transfers (to and from the device) will
be needed for velocity components as well.  The same is true for gradients.
The subroutines for calculation of gradients have OpenACC directives, and one
should insure that they work too.

Task 3: Port the solution of Navier-Stokes equations to GPUs.  As described
above, the first port to GPUs will have to be quite a big one, and the
question is how to take it?  Where to perform transfer of data?  All in the
Test_006 (good because all is in place, but the function would grow too much
and it might not be possible to assing all the details of numerical subroutines.
Alternative would be to make all calls for data transfer in the called functions
but that was requiring complex logic which was difficult to follow.

So, I decided for a combined approach:

- In the main function (represented by Test_006) transfer all the data which
will stay monolithic on the device (it won't change) such as linear system
matrices and gradient calculation matrices.  In addition, also reserve memory
on the device for data which will change on the device, such as velocity
components, pressure correction, gradient components, right hand side vector
and vectors which are a part of the CG linear solver (called p, q and r).
To avoid repetition, the reader is referred to Test_006.f90, all the lines
which end with ! <- GPU

- In the functions which perform tasks on GPUs, transfer data to the device
before each call to accelerated function, and fetch what was computed back
to the host.  This approach is rather pragmatic and entails the ackowledgement
that majority of the functions are still performed on the host (CPU).  Such
functions are actually only three in the current implementation:

  Process_Mod/Compute_Momentum and
  Process_Mod/Compute_Pressure
  Field_Mod/Grad_Pressure.f90

In the first two, the righ-hand side estimated on the host (in all other pro-
cedures in Process_Mod except these two) has to be transfered to the device
before a call to CG solver, and the solution from the solver has to be
transferred back to host.  But, one should be careful about the computation
of momentum, because velocity components calculated on the device are not
the same as those on the host after velocity correction.  Therefore, to en-
sure a convergence history on the GPUs as simular as possible on the CPU,
one should also transfer the lates value of corrected velocity components
to the device, since they serve as initial guess to the CG solver.

The third one, Grad_Pressure from Field_Mod also performs calculations on
GPUs, and for it to work properly, one has to update the device with the
latest values of variables, and update host with calculated gradient com-
ponents.

All the changes mentioned above are designated with the symbol ! <- GPU_1
starting around column 65.

Task 4: run the simulations on GPUs.  In this step, I compiled the program
with a simple 'make' command, without any options passed, ran the simulations
and obtained the following results:

 #=================================================================================#
 #                              CPU usage statistics                               #
 #---------------------------------------------------------------------------------#
 #                      Total CPU time: 000:00:53 [hhh:mm:ss]                      #
 #---------------------------------------------+-----------------------------------#
 #        Description of the activity:         |            Spent time:            #
 #---------------------------------------------+-----------------+-----------------#
 # - CG_for_Pressure                           |    13.40 [s]    |     24.93 %     #
 # - Save_Vtk_Vector                           |    12.74 [s]    |     23.71 %     #
 # - Test_006                                  |     5.75 [s]    |     10.69 %     #
 # - Save_Vtk_Scalar                           |     4.59 [s]    |      8.54 %     #
 # - Insert_Diffusion_Bc                       |     3.50 [s]    |      6.51 %     #
 # - Correct_Velocity                          |     3.34 [s]    |      6.21 %     #
 # - Insert_Volume_Source_For_Pressure         |     3.25 [s]    |      6.05 %     #
 # - Add_Advection_Term                        |     2.45 [s]    |      4.55 %     #
 # - Grad_Pressure                             |     1.71 [s]    |      3.19 %     #
 # - Compute_Momentum                          |     0.82 [s]    |      1.53 %     #
 # - CG_for_Momentum                           |     0.52 [s]    |      0.96 %     #
 # - Form_Diffusion_Matrix                     |     0.45 [s]    |      0.83 %     #
 # - Add_Pressure_Term                         |     0.32 [s]    |      0.60 %     #
 # - Add_Inertial_Term                         |     0.32 [s]    |      0.60 %     #
 # - Calculate_Grad_Matrix                     |     0.24 [s]    |      0.44 %     #
 # - Compute_Pressure                          |     0.18 [s]    |      0.34 %     #
 # - Form_Pressure_Matrix                      |     0.17 [s]    |      0.32 %     #
 #---------------------------------------------+-----------------+-----------------#

The profiling data shows that the code runs six time faster on GPUs, and that
linear solver for pressure alone went from 236 to 13 seconds, a very good
improvement.
